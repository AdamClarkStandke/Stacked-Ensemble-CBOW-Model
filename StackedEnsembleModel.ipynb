{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "LR = 3e-4  \n",
    "BATCH_SIZE_TWO = 1\n",
    "TOLERENCE = 5e-2\n",
    "HIDDEN =20\n",
    "MEMBERS = 3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import re\n",
    "import string\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_text,_label, _len) in batch:\n",
    "        label_list.append(_label)\n",
    "        length_list.append(_len)\n",
    "        tensor = torch.tensor(_text, dtype=torch.long)\n",
    "        text_list.append(tensor)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float)\n",
    "    length_list = torch.tensor(length_list)\n",
    "    return text_list,label_list, length_list\n",
    "\n",
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.data = pd.read_pickle(file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data.vector[idx]\n",
    "        lens = self.data.lengths[idx]\n",
    "        y = self.data.label[idx]\n",
    "        return X,y,lens\n",
    "    \n",
    "testing = VectorizeData('variable_test_set.csv')\n",
    "dtes_load = DataLoader(testing, batch_size=BATCH_SIZE_TWO, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loading the pretrained embedding weights'''\n",
    "weights=torch.load('CBOW_NEWS.pth')\n",
    "pre_trained = nn.Embedding.from_pretrained(weights)\n",
    "pre_trained.weight.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemntation of Integrated Stacking Model as detailed by Jason Brownlee @ https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def create_emb_layer(pre_trained):\n",
    "    num_embeddings = pre_trained.num_embeddings\n",
    "    embedding_dim = pre_trained.embedding_dim\n",
    "    emb_layer = nn.Embedding.from_pretrained(pre_trained.weight.data, freeze=True)\n",
    "    return emb_layer, embedding_dim\n",
    "\n",
    "class OneLayerGRUAttModel(nn.Module):\n",
    "    def __init__(self, pre_trained, HIDDEN, num_labels):\n",
    "        super(OneLayerGRUAttModel, self).__init__()\n",
    "        self.n_class = num_labels\n",
    "        self.embedding, self.embedding_dim = create_emb_layer(pre_trained)\n",
    "        self.gru = nn.GRU(self.embedding_dim, hidden_size=HIDDEN, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.label = nn.Linear(2*HIDDEN, self.n_class)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def attention_net(self, gru_output, final_state):\n",
    "        hidden = final_state\n",
    "        output = gru_output[0]\n",
    "        attn_weights = torch.matmul(output, hidden.transpose(1, 0))\n",
    "        soft_attn_weights = F.softmax(attn_weights.transpose(1, 0), dim=1)\n",
    "        new_hidden_state = torch.matmul(output.transpose(1,0), soft_attn_weights.transpose(1,0))\n",
    "        return new_hidden_state.transpose(1, 0)\n",
    "    \n",
    "    def forward(self, x, text_len):\n",
    "        embeds = self.embedding(x)\n",
    "        pack = pack_padded_sequence(embeds, text_len, batch_first=True, enforce_sorted=False)\n",
    "        output, hidden = self.gru(pack)\n",
    "        hidden = torch.cat((hidden[0,:, :], hidden[1,:, :]), dim=1)\n",
    "        attn_output = self.attention_net(output, hidden)\n",
    "        logits = self.label(attn_output)\n",
    "        outputs = self.act(logits.view(-1))\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class TwoLayerGRUAttModel(nn.Module):\n",
    "    def __init__(self, pre_trained, HIDDEN, num_labels):\n",
    "        super(TwoLayerGRUAttModel, self).__init__()\n",
    "        self.n_class = num_labels\n",
    "        self.embedding, self.embedding_dim = create_emb_layer(pre_trained)\n",
    "        self.gru = nn.GRU(self.embedding_dim, hidden_size=HIDDEN, num_layers=2,batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        self.label = nn.Linear(2*HIDDEN, self.n_class)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def attention_net(self, gru_output, final_state):\n",
    "        hidden = final_state\n",
    "        output = gru_output[0]\n",
    "        attn_weights = torch.matmul(output, hidden.transpose(1, 0))\n",
    "        soft_attn_weights = F.softmax(attn_weights.transpose(1, 0), dim=1)\n",
    "        new_hidden_state = torch.matmul(output.transpose(1,0), soft_attn_weights.transpose(1,0))\n",
    "        return new_hidden_state.transpose(1, 0)\n",
    "    \n",
    "    def forward(self, x, text_len):\n",
    "        embeds = self.embedding(x)\n",
    "        pack = pack_padded_sequence(embeds, text_len, batch_first=True, enforce_sorted=False)\n",
    "        output, hidden = self.gru(pack)\n",
    "        hidden = torch.cat((hidden[0,:, :], hidden[1,:, :]), dim=1)\n",
    "        attn_output = self.attention_net(output, hidden)\n",
    "        logits = self.label(attn_output)\n",
    "        outputs = self.act(logits.view(-1))\n",
    "        return outputs  \n",
    "    \n",
    "class C_DNN(nn.Module):\n",
    "    def __init__(self, pre_trained,num_labels):\n",
    "        super(C_DNN, self).__init__()\n",
    "        self.n_class = num_labels\n",
    "        self.embedding, self.embedding_dim = create_emb_layer(pre_trained)\n",
    "        self.conv1D = nn.Conv2d(1, 100, kernel_size=(3,16), padding=(1,0))\n",
    "        self.label = nn.Linear(100, self.n_class)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.unsqueeze(1)\n",
    "        conv1d = self.conv1D(embeds)\n",
    "        relu = F.relu(conv1d).squeeze(3)\n",
    "        maxpool = F.max_pool1d(input=relu, kernel_size=relu.size(2)).squeeze(2)\n",
    "        fc = self.label(maxpool)\n",
    "        sig = self.act(fc)\n",
    "        return sig.squeeze(1)\n",
    "    \n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self, modelA, modelB, modelC):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.modelC = modelC\n",
    "        self.fc1 = nn.Linear(3, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, text, length):\n",
    "        x1=self.modelA(text, length) \n",
    "        x2=self.modelB(text,length)\n",
    "        x3=self.modelC(text)\n",
    "        x4 = torch.cat((x1.detach(),x2.detach(), x3.detach()), dim=0)\n",
    "        x5 = F.relu(self.fc1(x4))\n",
    "        output = self.act(self.fc2(x5))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(n_models):\n",
    "    all_models = []\n",
    "    for i in range(n_models):\n",
    "        filename = \"models/model_\"+str(i+1)+'.pth'\n",
    "        if filename == \"models/model_1.pth\": \n",
    "            model_one = OneLayerGRUAttModel(pre_trained, HIDDEN, 1)\n",
    "            model_one.load_state_dict(torch.load(filename))\n",
    "            #for param in model_one.parameters():\n",
    "                #param.requires_grad = False\n",
    "            all_models.append(model_one)\n",
    "        elif filename == \"models/model_2.pth\":\n",
    "            model_two = TwoLayerGRUAttModel(pre_trained, HIDDEN, 1)\n",
    "            model_two.load_state_dict(torch.load(filename))\n",
    "            #for param in model_two.parameters():\n",
    "                #param.requires_grad = False\n",
    "            all_models.append(model_two)\n",
    "        else:\n",
    "            model = C_DNN(pre_trained=pre_trained, num_labels=1)\n",
    "            model.load_state_dict(torch.load(filename))\n",
    "            #for param in model.parameters():\n",
    "                #param.requires_grad = False\n",
    "            all_models.append(model)\n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Idx: 1, Stacked Training Loss: 0.6144, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 2, Stacked Training Loss: 0.6143, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 3, Stacked Training Loss: 0.6141, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 4, Stacked Training Loss: 0.7601, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 5, Stacked Training Loss: 0.7792, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 6, Stacked Training Loss: 0.7629, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 7, Stacked Training Loss: 0.6271, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 8, Stacked Training Loss: 0.7618, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 9, Stacked Training Loss: 0.7580, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 10, Stacked Training Loss: 0.7590, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 11, Stacked Training Loss: 0.7790, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 12, Stacked Training Loss: 0.7588, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 13, Stacked Training Loss: 0.7631, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 14, Stacked Training Loss: 0.6143, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 15, Stacked Training Loss: 0.7786, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 16, Stacked Training Loss: 0.6145, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 17, Stacked Training Loss: 0.7785, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 18, Stacked Training Loss: 0.6146, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 19, Stacked Training Loss: 0.7579, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 20, Stacked Training Loss: 0.6147, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 21, Stacked Training Loss: 0.7611, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 22, Stacked Training Loss: 0.7581, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 23, Stacked Training Loss: 0.6254, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 24, Stacked Training Loss: 0.6148, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 25, Stacked Training Loss: 0.6149, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 26, Stacked Training Loss: 0.7569, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 27, Stacked Training Loss: 0.7781, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 28, Stacked Training Loss: 0.7597, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 29, Stacked Training Loss: 0.7568, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 30, Stacked Training Loss: 0.6150, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 31, Stacked Training Loss: 0.7574, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 32, Stacked Training Loss: 0.7552, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 33, Stacked Training Loss: 0.6316, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 34, Stacked Training Loss: 0.6152, Stacked Training Accuracy:%\n",
      "Epoch: 1, Idx: 35, Stacked Training Loss: 0.7777, Stacked Training Accuracy:%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d4af685201da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mvalidate_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtes_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"models/model_metaLearner.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d4af685201da>\u001b[0m in \u001b[0;36mvalidate_meta\u001b[0;34m(dataloader, model, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ichglaubeya/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def validate_meta(dataloader, model, epoch):\n",
    "    #initialize every epoch \n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    steps = 0\n",
    "    #set the model in training phase\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(dataloader): \n",
    "        text,label,lengths = batch \n",
    "        optimizer.zero_grad() \n",
    "        prediction = model(text, lengths) \n",
    "        loss = criterion(prediction, label) \n",
    "        acc = binary_accuracy(prediction, label)\n",
    "        \n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()  \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        #update the weights\n",
    "        optimizer.step()  \n",
    "        steps += 1\n",
    "        if steps % 1 == 0:\n",
    "            print(f'Epoch: {epoch}, Idx: {idx+1}, Stacked Training Loss: {loss.item():.4f}, Stacked Training Accuracy:%')\n",
    "        total_epoch_loss += loss.item()\n",
    "        if total_epoch_loss <= TOLERENCE:\n",
    "            return\n",
    "        total_epoch_acc += acc.item()\n",
    "\n",
    "models = load_all_models(MEMBERS)\n",
    "meta_model = MetaLearner(models[0], models[1], models[2])\n",
    "optimizer = optim.Adam(meta_model.parameters(), lr=LR)\n",
    "criterion = nn.BCELoss()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    validate_meta(dtes_load, meta_model, epoch)\n",
    "filename = \"models/model_metaLearner.pth\"\n",
    "torch.save(meta_model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneLayerGRUAttModel: tensor(0.7812)\n",
      "TwoLayerGRUAttModel: tensor(0.6250)\n",
      "C-DNNModel: tensor(0.3750)\n"
     ]
    }
   ],
   "source": [
    "def binary_accuracy(model, text, y, lens):\n",
    "    model.eval()\n",
    "    if lens != None:\n",
    "        preds = model(text, lens)\n",
    "    else:\n",
    "        preds = model(text)\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "batch=next(iter(dtes_load))\n",
    "text, label, length = batch\n",
    "\n",
    "for f in models:\n",
    "    if f.__class__.__name__=='OneLayerGRUAttModel':\n",
    "        acc=binary_accuracy(f, text, label, length)\n",
    "        print('OneLayerGRUAttModel: '+ str(acc))    \n",
    "    elif f.__class__.__name__=='TwoLayerGRUAttModel':\n",
    "        acc=binary_accuracy(f, text, label, length)\n",
    "        print('TwoLayerGRUAttModel: '+ str(acc))\n",
    "    else:\n",
    "        acc=binary_accuracy(f, text, label, None)\n",
    "        print('C-DNNModel: '+ str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
