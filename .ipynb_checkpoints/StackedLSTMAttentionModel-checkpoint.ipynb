{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "LR = 3e-4  \n",
    "HIDDEN = 20\n",
    "TOLERENCE= 1e-1\n",
    "BATCH_SIZE_TWO = 32\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import string\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loading the pretrained embedding weights'''\n",
    "weights=torch.load('CBOW_NEWS.pth')\n",
    "pre_trained = nn.Embedding.from_pretrained(weights)\n",
    "pre_trained.weight.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_text,_label, _len) in batch:\n",
    "        label_list.append(_label)\n",
    "        length_list.append(_len)\n",
    "        tensor = torch.tensor(_text, dtype=torch.long)\n",
    "        text_list.append(tensor)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float)\n",
    "    length_list = torch.tensor(length_list)\n",
    "    return text_list,label_list, length_list\n",
    "\n",
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.data = pd.read_pickle(file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data.vector[idx]\n",
    "        lens = self.data.lengths[idx]\n",
    "        y = self.data.label[idx]\n",
    "        return X,y,lens\n",
    "    \n",
    "    \n",
    "training = VectorizeData('variable_level_zero.csv')\n",
    "dt_load = DataLoader(training, batch_size=BATCH_SIZE_TWO, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part implementation of aravindpal's Text Classifier as found\n",
    "# @https://www.analyticsvidhya.com/blog/2020/01\n",
    "# /first-text-classification-in-pytorch/\n",
    "# and Prakash Pandey's LSTM+Attention model as found \n",
    "# @https://github.com/prakashpandey9\n",
    "# /Text-Classification-Pytorch/blob/master/models/LSTM_Attn.py\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def create_emb_layer(pre_trained):\n",
    "    num_embeddings = pre_trained.num_embeddings\n",
    "    embedding_dim = pre_trained.embedding_dim\n",
    "    emb_layer = nn.Embedding.from_pretrained(pre_trained.weight.data, freeze=True)\n",
    "    return emb_layer, embedding_dim\n",
    "\n",
    "class StackedLSTMAtteionModel(nn.Module):\n",
    "    def __init__(self, pre_trained,num_labels):\n",
    "        super(StackedLSTMAtteionModel, self).__init__()\n",
    "        self.n_class = num_labels\n",
    "        self.embedding, self.embedding_dim = create_emb_layer(pre_trained)\n",
    "        self.LSTM = nn.LSTM(self.embedding_dim, HIDDEN, num_layers=2,bidirectional=True,dropout=0.26,batch_first=True)\n",
    "        self.label = nn.Linear(2*HIDDEN, self.n_class)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def attention_net(self, Lstm_output, final_state):\n",
    "        hidden = final_state\n",
    "        output = Lstm_output[0]\n",
    "        attn_weights = torch.matmul(output, hidden.transpose(1, 0))\n",
    "        soft_attn_weights = F.softmax(attn_weights.transpose(1, 0), dim=1)\n",
    "        new_hidden_state = torch.matmul(output.transpose(1,0), soft_attn_weights.transpose(1,0))\n",
    "        return new_hidden_state.transpose(1, 0)\n",
    "    \n",
    "    def forward(self, x, text_len):\n",
    "        embeds = self.embedding(x)\n",
    "        pack = pack_padded_sequence(embeds, text_len, batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell) = self.LSTM(pack)\n",
    "        hidden = torch.cat((hidden[0,:, :], hidden[1,:, :]), dim=1)\n",
    "        attn_output = self.attention_net(output, hidden)\n",
    "        logits = self.label(attn_output)\n",
    "        outputs = self.act(logits.view(-1))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackedLSTMAtteionModel(pre_trained=pre_trained, num_labels=1)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def train(dataloader, model, epoch):\n",
    "    #initialize every epoch \n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    steps = 0\n",
    "    #set the model in training phase\n",
    "    model.train()  \n",
    "    for idx, batch in enumerate(dataloader): \n",
    "        text,label,lengths = batch\n",
    "        optimizer.zero_grad() \n",
    "        prediction = model(text, lengths)\n",
    "        loss = criterion(prediction, label) \n",
    "        acc = binary_accuracy(prediction, label)\n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        #update the weights\n",
    "        optimizer.step()  \n",
    "        steps += 1\n",
    "        if steps % 1  == 0:\n",
    "            print(f'Epoch: {epoch}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item():.2f}%')\n",
    "        total_epoch_loss = loss.item()\n",
    "        if total_epoch_loss <= TOLERENCE:\n",
    "            return True\n",
    "\n",
    "end_training = False\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    end_training=train(dt_load, model, epoch)\n",
    "    if end_training:\n",
    "        filename = \"models/model_\"+str(1)+'.pth'\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        break\n",
    "if not end_training:\n",
    "    filename = \"models/model_\"+str(1)+'.pth'\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
