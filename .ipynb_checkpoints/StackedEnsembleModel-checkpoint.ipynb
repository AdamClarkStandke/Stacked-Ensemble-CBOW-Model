{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "LR = 3e-4  \n",
    "BATCH_SIZE_TWO = 1\n",
    "TOLERENCE = 1e-1\n",
    "HIDDEN =20\n",
    "MEMBERS = 3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import re\n",
    "import string\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_text,_label, _len) in batch:\n",
    "        label_list.append(_label)\n",
    "        length_list.append(_len)\n",
    "        tensor = torch.tensor(_text, dtype=torch.long)\n",
    "        text_list.append(tensor)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float)\n",
    "    length_list = torch.tensor(length_list)\n",
    "    return text_list,label_list, length_list\n",
    "\n",
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.data = pd.read_pickle(file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data.vector[idx]\n",
    "        lens = self.data.lengths[idx]\n",
    "        y = self.data.label[idx]\n",
    "        return X,y,lens\n",
    "    \n",
    "testing = VectorizeData('variable_test_set.csv')\n",
    "dtes_load = DataLoader(testing, batch_size=BATCH_SIZE_TWO, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loading the pretrained embedding weights'''\n",
    "weights=torch.load('CBOW_NEWS.pth')\n",
    "pre_trained = nn.Embedding.from_pretrained(weights)\n",
    "pre_trained.weight.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part Implemntation of Integrated Stacking Model as detailed by \n",
    "# Jason Brownlee \n",
    "# @ https://machinelearningmastery.com/stacking-ensemble-for-deep-learning\n",
    "# -neural-networks/\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def create_emb_layer(pre_trained):\n",
    "    num_embeddings = pre_trained.num_embeddings\n",
    "    embedding_dim = pre_trained.embedding_dim\n",
    "    emb_layer = nn.Embedding.from_pretrained(pre_trained.weight.data, freeze=True)\n",
    "    return emb_layer, embedding_dim\n",
    "\n",
    "class StackedLSTMAtteionModel(nn.Module):\n",
    "    def __init__(self, pre_trained,num_labels):\n",
    "        super(StackedLSTMAtteionModel, self).__init__()\n",
    "        self.n_class = num_labels\n",
    "        self.embedding, self.embedding_dim = create_emb_layer(pre_trained)\n",
    "        self.LSTM = nn.LSTM(self.embedding_dim, HIDDEN, num_layers=2,bidirectional=True,dropout=0.26,batch_first=True)\n",
    "        self.label = nn.Linear(2*HIDDEN, self.n_class)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def attention_net(self, Lstm_output, final_state):\n",
    "        hidden = final_state\n",
    "        output = Lstm_output[0]\n",
    "        attn_weights = torch.matmul(output, hidden.transpose(1, 0))\n",
    "        soft_attn_weights = F.softmax(attn_weights.transpose(1, 0), dim=1)\n",
    "        new_hidden_state = torch.matmul(output.transpose(1,0), soft_attn_weights.transpose(1,0))\n",
    "        return new_hidden_state.transpose(1, 0)\n",
    "    \n",
    "    def forward(self, x, text_len):\n",
    "        embeds = self.embedding(x)\n",
    "        pack = pack_padded_sequence(embeds, text_len, batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell) = self.LSTM(pack)\n",
    "        hidden = torch.cat((hidden[0,:, :], hidden[1,:, :]), dim=1)\n",
    "        attn_output = self.attention_net(output, hidden)\n",
    "        logits = self.label(attn_output)\n",
    "        outputs = self.act(logits.view(-1))\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class TwoLayerGRUAttModel(nn.Module):\n",
    "    def __init__(self, pre_trained, HIDDEN, num_labels):\n",
    "        super(TwoLayerGRUAttModel, self).__init__()\n",
    "        self.n_class = num_labels\n",
    "        self.embedding, self.embedding_dim = create_emb_layer(pre_trained)\n",
    "        self.gru = nn.GRU(self.embedding_dim, hidden_size=HIDDEN, num_layers=2,batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        self.label = nn.Linear(2*HIDDEN, self.n_class)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def attention_net(self, gru_output, final_state):\n",
    "        hidden = final_state\n",
    "        output = gru_output[0]\n",
    "        attn_weights = torch.matmul(output, hidden.transpose(1, 0))\n",
    "        soft_attn_weights = F.softmax(attn_weights.transpose(1, 0), dim=1)\n",
    "        new_hidden_state = torch.matmul(output.transpose(1,0), soft_attn_weights.transpose(1,0))\n",
    "        return new_hidden_state.transpose(1, 0)\n",
    "    \n",
    "    def forward(self, x, text_len):\n",
    "        embeds = self.embedding(x)\n",
    "        pack = pack_padded_sequence(embeds, text_len, batch_first=True, enforce_sorted=False)\n",
    "        output, hidden = self.gru(pack)\n",
    "        hidden = torch.cat((hidden[0,:, :], hidden[1,:, :]), dim=1)\n",
    "        attn_output = self.attention_net(output, hidden)\n",
    "        logits = self.label(attn_output)\n",
    "        outputs = self.act(logits.view(-1))\n",
    "        return outputs  \n",
    "    \n",
    "class C_DNN(nn.Module):\n",
    "    def __init__(self, pre_trained,num_labels):\n",
    "        super(C_DNN, self).__init__()\n",
    "        self.n_class = num_labels\n",
    "        self.embedding, self.embedding_dim = create_emb_layer(pre_trained)\n",
    "        self.conv1D = nn.Conv2d(1, 100, kernel_size=(3,16), padding=(1,0))\n",
    "        self.label = nn.Linear(100, self.n_class)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.unsqueeze(1)\n",
    "        conv1d = self.conv1D(embeds)\n",
    "        relu = F.relu(conv1d).squeeze(3)\n",
    "        maxpool = F.max_pool1d(input=relu, kernel_size=relu.size(2)).squeeze(2)\n",
    "        fc = self.label(maxpool)\n",
    "        sig = self.act(fc)\n",
    "        return sig.squeeze(1)\n",
    "    \n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self, modelA, modelB, modelC):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.modelC = modelC\n",
    "        self.fc1 = nn.Linear(3, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, text, length):\n",
    "        x1=self.modelA(text, length) \n",
    "        x2=self.modelB(text,length)\n",
    "        x3=self.modelC(text)\n",
    "        x4 = torch.cat((x1.detach(),x2.detach(), x3.detach()), dim=0)\n",
    "        x5 = F.relu(self.fc1(x4))\n",
    "        output = self.act(self.fc2(x5))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(n_models):\n",
    "    all_models = []\n",
    "    for i in range(n_models):\n",
    "        filename = \"models/model_\"+str(i+1)+'.pth'\n",
    "        if filename == \"models/model_1.pth\": \n",
    "            model_one = StackedLSTMAtteionModel(pre_trained, 1)\n",
    "            model_one.load_state_dict(torch.load(filename))\n",
    "            for param in model_one.parameters():\n",
    "                param.requires_grad = False\n",
    "            all_models.append(model_one)\n",
    "        elif filename == \"models/model_2.pth\":\n",
    "            model_two = TwoLayerGRUAttModel(pre_trained, HIDDEN, 1)\n",
    "            model_two.load_state_dict(torch.load(filename))\n",
    "            for param in model_two.parameters():\n",
    "                param.requires_grad = False\n",
    "            all_models.append(model_two)\n",
    "        else:\n",
    "            model = C_DNN(pre_trained=pre_trained, num_labels=1)\n",
    "            model.load_state_dict(torch.load(filename))\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            all_models.append(model)\n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_all_models(MEMBERS)\n",
    "meta_model = MetaLearner(models[0], models[1], models[2])\n",
    "optimizer = optim.Adam(meta_model.parameters(), lr=LR)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "def validate_meta(dataloader, model, epoch):\n",
    "    #initialize every epoch \n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    steps = 0\n",
    "    #set the model in training phase\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(dataloader): \n",
    "        text,label,lengths = batch \n",
    "        optimizer.zero_grad() \n",
    "        prediction = model(text, lengths) \n",
    "        loss = criterion(prediction, label) \n",
    "        acc = binary_accuracy(prediction, label)\n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()  \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        #update the weights\n",
    "        optimizer.step()  \n",
    "        steps += 1\n",
    "        if steps % 1 == 0:\n",
    "            print(f'Epoch: {epoch}, Idx: {idx+1}, Meta Training Loss: {loss.item():.4f}, Meta Training Accuracy:{acc.item():.2f}%')\n",
    "        epoch_loss = loss.item()\n",
    "        if epoch_loss < TOLERENCE:\n",
    "            return True\n",
    "\n",
    "end_training = False\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    end_training=validate_meta(dtes_load, meta_model, epoch)\n",
    "    if end_training:\n",
    "        filename = \"models/model_metaLearner.pth\"\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        break\n",
    "if not end_training:\n",
    "    filename = \"models/model_metaLearner.pth\"\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
