{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, context_file, target_file):\n",
    "        self.context = torch.tensor(genfromtxt(context_file, delimiter=',', dtype=np.int64)) \n",
    "        self.target =  torch.tensor(genfromtxt(target_file, delimiter=',', dtype=np.int64))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.context[idx], self.target[idx])\n",
    "\n",
    "vocab_obj = torch.load('vocab_obj.pth')\n",
    "# indextow = torch.load('indexDict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = '/home/ichglaubeya/projects/NLP_Projects/Spacy/fake_or_real/target/'\n",
    "context_path = '/home/ichglaubeya/projects/NLP_Projects/Spacy/fake_or_real/CBOW/'\n",
    "targetlist = [t for t in os.listdir(target_path)]\n",
    "contextlist = [c for c in os.listdir(context_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_objects = []\n",
    "for file_one, file_two in zip(contextlist, targetlist):\n",
    "    dirpath_one=os.path.join(context_path+file_one)\n",
    "    dirpath_two=os.path.join(target_path+file_two)\n",
    "    data_objects.append(CustomTextDataset(dirpath_one,dirpath_two))\n",
    "print('Data has been loaded, starting Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training CBOW Embedding Model'''\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        mean = torch.mean(embeds, dim=1,keepdim=True)\n",
    "        y_hat = self.fc(mean)\n",
    "        y_hat = torch.squeeze(y_hat, dim=1)\n",
    "        log_probs = F.log_softmax(y_hat, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    loss_function = nn.NLLLoss()\n",
    "    total_loss = 0\n",
    "    log_interval = 1\n",
    "    for idx, sample in enumerate(data):\n",
    "        batch_one = sample\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(batch_one.context)\n",
    "        loss = loss_function(log_probs, batch_one.target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| loss {:6f} '.format(epoch, idx, len(data), total_loss))\n",
    "\n",
    "EPOCHS = 5 # epoch\n",
    "EMBEDDING_DIM = 16\n",
    "VOCAB_SIZE = len(vocab_obj.values())\n",
    "\n",
    "model = CBOW(VOCAB_SIZE,EMBEDDING_DIM)\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, data_objects)\n",
    "torch.save(model.embeddings.weight, 'CBOW_NEWS.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
