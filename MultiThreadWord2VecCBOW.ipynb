{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from torchtext.legacy import data \n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.multiprocessing as mp\n",
    "from multiprocessing import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, context_file, target_file):\n",
    "        self.context = genfromtxt(context_file, delimiter=',', dtype=np.int64) \n",
    "        self.target =  genfromtxt(target_file, delimiter=',', dtype=np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.context[idx], self.target[idx])\n",
    "\n",
    "temp_data = CustomTextDataset('CBOW/context.csv', 'CBOW/target.csv')\n",
    "vocab_obj = torch.load('CBOW/vocab_obj.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training CBOW Embedding Model'''\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        mean = torch.mean(embeds, dim=1,keepdim=True)\n",
    "        y_hat = self.fc(mean)\n",
    "        y_hat = torch.squeeze(y_hat, dim=1)\n",
    "        log_probs = F.log_softmax(y_hat, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 |■| avg_loss:0Epoch:2 |■■| avg_loss:45.49118947982788Epoch:3 |■■■| avg_loss:45.399390161037445Epoch:4 |■■■■| avg_loss:45.30847100416819Epoch:5 |■■■■■| avg_loss:45.216905385255814"
     ]
    }
   ],
   "source": [
    "Q = Queue()\n",
    "def train(model, data):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    loss_function = nn.NLLLoss()\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    for idx, sample in enumerate(data):\n",
    "        batch_one = sample\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(batch_one[0])\n",
    "        loss = loss_function(log_probs, batch_one[1])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        losses.append(total_loss)\n",
    "    Q.put(losses[-1])\n",
    "\n",
    "num_processes = 4\n",
    "EPOCHS = 5 # epoch\n",
    "EMBEDDING_DIM = 16\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab_obj.values())\n",
    "\n",
    "model = CBOW(VOCAB_SIZE,EMBEDDING_DIM)\n",
    "model.share_memory()\n",
    "temp = []\n",
    "loss = 0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print('Epoch:'+str(epoch)+' |'+'■'*epoch+'| avg_loss:'+str(loss), end='')\n",
    "    os.system(\"printf '\\033c'\")\n",
    "    processes = []\n",
    "    for rank in range(num_processes):\n",
    "        data_loader = DataLoader(temp_data, sampler=DistributedSampler(dataset=temp_data,num_replicas=num_processes,rank=rank), batch_size=BATCH_SIZE)\n",
    "        p = mp.Process(target=train, args=(model, data_loader))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        temp.append(Q.get())\n",
    "        p.join()\n",
    "    loss=sum(temp)/len(temp)\n",
    "torch.save(model.embeddings.weight, 'CBOW_NEWS.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
