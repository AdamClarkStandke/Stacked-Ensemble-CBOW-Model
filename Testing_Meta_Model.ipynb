{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "LR = 3e-4  \n",
    "BATCH_SIZE_TWO = 1\n",
    "HIDDEN =20\n",
    "MEMBERS = 3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import re\n",
    "import string\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_text,_label, _len) in batch:\n",
    "        label_list.append(_label)\n",
    "        length_list.append(_len)\n",
    "        tensor = torch.tensor(_text, dtype=torch.long)\n",
    "        text_list.append(tensor)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float)\n",
    "    length_list = torch.tensor(length_list)\n",
    "    return text_list,label_list, length_list\n",
    "\n",
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.data = pd.read_pickle(file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data.vector[idx]\n",
    "        lens = self.data.lengths[idx]\n",
    "        y = self.data.label[idx]\n",
    "        return X,y,lens\n",
    "    \n",
    "testing = VectorizeData('predict_set.csv')\n",
    "prediction = DataLoader(testing, batch_size=BATCH_SIZE_TWO, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loading the pretrained embedding weights'''\n",
    "weights=torch.load('CBOW_NEWS.pth')\n",
    "pre_trained = nn.Embedding.from_pretrained(weights)\n",
    "pre_trained.weight.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(pre_trained):\n",
    "    num_embeddings = pre_trained.num_embeddings\n",
    "    embedding_dim = pre_trained.embedding_dim\n",
    "    emb_layer = nn.Embedding.from_pretrained(pre_trained.weight.data, freeze=True)\n",
    "    return emb_layer, embedding_dim\n",
    "\n",
    "class StackedLSTMAtteionModel(nn.Module):\n",
    "    def __init__(self, pre_trained,num_labels):\n",
    "        super(StackedLSTMAtteionModel, self).__init__()\n",
    "        self.n_class = num_labels\n",
    "        self.embedding, self.embedding_dim = create_emb_layer(pre_trained)\n",
    "        self.LSTM = nn.LSTM(self.embedding_dim, HIDDEN, num_layers=2,bidirectional=True,dropout=0.26,batch_first=True)\n",
    "        self.label = nn.Linear(2*HIDDEN, self.n_class)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def attention_net(self, Lstm_output, final_state):\n",
    "        hidden = final_state\n",
    "        output = Lstm_output[0]\n",
    "        attn_weights = torch.matmul(output, hidden.transpose(1, 0))\n",
    "        soft_attn_weights = F.softmax(attn_weights.transpose(1, 0), dim=1)\n",
    "        new_hidden_state = torch.matmul(output.transpose(1,0), soft_attn_weights.transpose(1,0))\n",
    "        return new_hidden_state.transpose(1, 0)\n",
    "    \n",
    "    def forward(self, x, text_len):\n",
    "        embeds = self.embedding(x)\n",
    "        pack = pack_padded_sequence(embeds, text_len, batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell) = self.LSTM(pack)\n",
    "        hidden = torch.cat((hidden[0,:, :], hidden[1,:, :]), dim=1)\n",
    "        attn_output = self.attention_net(output, hidden)\n",
    "        logits = self.label(attn_output)\n",
    "        outputs = self.act(logits.view(-1))\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class TwoLayerGRUAttModel(nn.Module):\n",
    "    def __init__(self, pre_trained, HIDDEN, num_labels):\n",
    "        super(TwoLayerGRUAttModel, self).__init__()\n",
    "        self.n_class = num_labels\n",
    "        self.embedding, self.embedding_dim = create_emb_layer(pre_trained)\n",
    "        self.gru = nn.GRU(self.embedding_dim, hidden_size=HIDDEN, num_layers=2,batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        self.label = nn.Linear(2*HIDDEN, self.n_class)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def attention_net(self, gru_output, final_state):\n",
    "        hidden = final_state\n",
    "        output = gru_output[0]\n",
    "        attn_weights = torch.matmul(output, hidden.transpose(1, 0))\n",
    "        soft_attn_weights = F.softmax(attn_weights.transpose(1, 0), dim=1)\n",
    "        new_hidden_state = torch.matmul(output.transpose(1,0), soft_attn_weights.transpose(1,0))\n",
    "        return new_hidden_state.transpose(1, 0)\n",
    "    \n",
    "    def forward(self, x, text_len):\n",
    "        embeds = self.embedding(x)\n",
    "        pack = pack_padded_sequence(embeds, text_len, batch_first=True, enforce_sorted=False)\n",
    "        output, hidden = self.gru(pack)\n",
    "        hidden = torch.cat((hidden[0,:, :], hidden[1,:, :]), dim=1)\n",
    "        attn_output = self.attention_net(output, hidden)\n",
    "        logits = self.label(attn_output)\n",
    "        outputs = self.act(logits.view(-1))\n",
    "        return outputs  \n",
    "    \n",
    "class C_DNN(nn.Module):\n",
    "    def __init__(self, pre_trained,num_labels):\n",
    "        super(C_DNN, self).__init__()\n",
    "        self.n_class = num_labels\n",
    "        self.embedding, self.embedding_dim = create_emb_layer(pre_trained)\n",
    "        self.conv1D = nn.Conv2d(1, 100, kernel_size=(3,16), padding=(1,0))\n",
    "        self.label = nn.Linear(100, self.n_class)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.unsqueeze(1)\n",
    "        conv1d = self.conv1D(embeds)\n",
    "        relu = F.relu(conv1d).squeeze(3)\n",
    "        maxpool = F.max_pool1d(input=relu, kernel_size=relu.size(2)).squeeze(2)\n",
    "        fc = self.label(maxpool)\n",
    "        sig = self.act(fc)\n",
    "        return sig.squeeze(1)\n",
    "    \n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self, modelA, modelB, modelC):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.modelC = modelC\n",
    "        self.fc1 = nn.Linear(3, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, text, length):\n",
    "        x1=self.modelA(text, length) \n",
    "        x2=self.modelB(text,length)\n",
    "        x3=self.modelC(text)\n",
    "        x4 = torch.cat((x1.detach(),x2.detach(), x3.detach()), dim=0)\n",
    "        x5 = F.relu(self.fc1(x4))\n",
    "        output = self.act(self.fc2(x5))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(n_models):\n",
    "    all_models = []\n",
    "    for i in range(n_models):\n",
    "        filename = \"models/model_\"+str(i+1)+'.pth'\n",
    "        if filename == \"models/model_1.pth\": \n",
    "            model_one = StackedLSTMAtteionModel(pre_trained, 1)\n",
    "            model_one.load_state_dict(torch.load(filename))\n",
    "            for param in model_one.parameters():\n",
    "                param.requires_grad = False\n",
    "            all_models.append(model_one)\n",
    "        elif filename == \"models/model_2.pth\":\n",
    "            model_two = TwoLayerGRUAttModel(pre_trained, HIDDEN, 1)\n",
    "            model_two.load_state_dict(torch.load(filename))\n",
    "            for param in model_two.parameters():\n",
    "                param.requires_grad = False\n",
    "            all_models.append(model_two)\n",
    "        else:\n",
    "            model = C_DNN(pre_trained=pre_trained, num_labels=1)\n",
    "            model.load_state_dict(torch.load(filename))\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            all_models.append(model)\n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Loading the meta_model'''\n",
    "filename=\"models/model_metaLearner.pth\"\n",
    "models = load_all_models(MEMBERS)\n",
    "meta_model = MetaLearner(models[0], models[1], models[2])\n",
    "meta_model.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MetaLearner                              --\n",
       "├─StackedLSTMAtteionModel: 1-1           --\n",
       "│    └─Embedding: 2-1                    (1,744,400)\n",
       "│    └─LSTM: 2-2                         (16,000)\n",
       "│    └─Linear: 2-3                       (41)\n",
       "│    └─Sigmoid: 2-4                      --\n",
       "├─TwoLayerGRUAttModel: 1-2               --\n",
       "│    └─Embedding: 2-5                    (1,744,400)\n",
       "│    └─GRU: 2-6                          (12,000)\n",
       "│    └─Linear: 2-7                       (41)\n",
       "│    └─Sigmoid: 2-8                      --\n",
       "├─C_DNN: 1-3                             --\n",
       "│    └─Embedding: 2-9                    (1,744,400)\n",
       "│    └─Conv2d: 2-10                      (4,900)\n",
       "│    └─Linear: 2-11                      (101)\n",
       "│    └─Sigmoid: 2-12                     --\n",
       "├─Linear: 1-4                            8\n",
       "├─Linear: 1-5                            3\n",
       "├─Sigmoid: 1-6                           --\n",
       "=================================================================\n",
       "Total params: 5,266,294\n",
       "Trainable params: 11\n",
       "Non-trainable params: 5,266,283\n",
       "================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(meta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy:    0.898\n"
     ]
    }
   ],
   "source": [
    "def binary_accuracy(dataloader, model):\n",
    "    #round predictions to the closest integer\n",
    "    correct = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (text,label,lengths) in enumerate(dataloader):\n",
    "            rounded_preds = torch.round(model(text, lengths))\n",
    "            correct.append((rounded_preds == label).float()) \n",
    "        acc = sum(correct)/len(correct)\n",
    "    return acc\n",
    "\n",
    "print('Checking the results of test dataset.')\n",
    "accu_test = binary_accuracy(prediction, meta_model)\n",
    "print(f'test accuracy: {accu_test.item():8.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
